{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network and Jane Austen\n",
    "\n",
    "Author: Greg Strabel\n",
    "\n",
    "This notebook trains a simple recurrent neural network (single layer of LSTM units) to read a sequence of unicode characters and then predict the next unicode character in the sequence. The corpus for training the network is comprised of overlapping sequences of text from Jane Austen's Sense and Sensibility. The code in this notebook is based on the example found [here](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py). Given the amount of time required to train even a single epoch of the model on a CPU, for the sake of the reader I have saved a copy of the neural network produced after 5 training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense and Sensibility by Jane Austen 1811]\n",
      "\n",
      "CHAPTER 1\n",
      "\n",
      "\n",
      "The family of Dashwood had long been settled in Sussex.\n",
      "Their estate was large, and their residence was at Norland Park,\n",
      "in the centre of their property, where, for many generations,\n",
      "they had lived in so respectable a manner as to engage\n",
      "the general good opinion of their surrounding acquaintance.\n",
      "The late owner of this estate was a single man, who lived\n",
      "to a very advanced age, and who for many years of his life,\n",
      "had a constant companion and housekeeper in his sister.\n",
      "But her death, which happened ten years before his own,\n",
      "produced a great alteration in his home; for to supply\n",
      "her loss, he invited and received into his house the family\n",
      "of his nephew Mr. Henry Dashwood, the legal inheritor\n",
      "of the Norland estate, and the person to whom he intended\n",
      "to bequeath it.  In the society of his nephew and niece,\n",
      "and their children, the old Gentleman's days were\n",
      "comfortably spent.  His attachment to them all increased.\n",
      "The constant attention \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Get the text of Jane Austen's Sense and Sensibility from Project Gutenberg\n",
    "text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct characters in training corpus: 78\n"
     ]
    }
   ],
   "source": [
    "print('Number of distinct characters in training corpus: {}'.format(len(chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I construct a simple recurrent neural network comprised of a single layer of 128 LSTM units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 224324\n",
      "Vectorization...\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 50\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_to_ix[char]] = 1\n",
    "    y[i, char_to_ix[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "from keras.models import load_model\n",
    "model = load_model(cwd + '\\\\JaneAustenKerasModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the model for two epochs at a time. After each epoch, we provide the RNN a seed of 50 unicode characters and ask it to generate the next 400 unicode characters. Rather than have the RNN do this just once for each epoch, we have the RNN generate unicode sequences for four different levels of sampling diversity. The intuition behind using different levels of sampling diversity is similar to that behind simulated annealing. Given a sequence of unicode characters, the RNN estimates the probability of each unicode character coming next in the sequence. To generate a sequence of unicode characters, we are asking the RNN to sample from this distribution. The RNN is effectively sampling from a search space in the same way that one would in stochastic gradient descent. As in simulated annealing, we may be able to achieve superior results by allowing the RNN to explore the search space more or less freely; intuitively, this is the goal of testing different sampling diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "224324/224324 [==============================] - 620s - loss: 1.2879   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"sewhere, it could not be\n",
      "in THAT quarter.  'THERE,\"\n",
      "sewhere, it could not be\n",
      "in THAT quarter.  'THERE, and the possible to be the world to her sister, and the same and some of the world to the same to be the more than that the could every thing to her sister, and the same time to be the same to be the particulare of the particular\n",
      "of the same to the three of the same and the same to the comfort any one of the particular\n",
      "of the conscier, and her sister,\n",
      "and the particular sincered to the more than \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"sewhere, it could not be\n",
      "in THAT quarter.  'THERE,\"\n",
      "sewhere, it could not be\n",
      "in THAT quarter.  'THERE, and in the increasing to have herself, and the pardon accommonable, and in the every at her sister and declared to the conscience, her sister's disposition, and they were without her own than that I would be united to the attinument\n",
      "was a more than not before him to speak to her and a plan in the truth the from his sister, the than leave the same one, that her ingreaties,\n",
      "though she was every mar\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"sewhere, it could not be\n",
      "in THAT quarter.  'THERE,\"\n",
      "sewhere, it could not be\n",
      "in THAT quarter.  'THERE,\n",
      "her liberal comfort as soon wish,\n",
      "farther been great great riesicly towards in his sister, and id ne so lady for it Marianne would had\n",
      "notiessing encou"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gregory\\Anaconda2\\envs\\python35\\lib\\site-packages\\ipykernel\\__main__.py:41: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raged her o, she was peruacy in the bealment as my sisted, great continued, and could talked Willoughby was, where must been one early at her wos.\n",
      "But our miam brother by my aparrponcy.\"\n",
      "\n",
      "\"I am\n",
      "she could be so much hinces, to spirits fartable,\n",
      "with\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"sewhere, it could not be\n",
      "in THAT quarter.  'THERE,\"\n",
      "sewhere, it could not be\n",
      "in THAT quarter.  'THERE,\n",
      "and was as no many rousingly\n",
      "answer.\n",
      "But well-esudeed risking hroming soatiwity\n",
      "in sister's tably, \"I go repity that seemed the glad them to\n",
      "gisted themsemb reell a cautitomablesf from Ma. Oh, you rep as Miss Thousate, that Sir John.\"\n",
      "\n",
      "\"It cle;svis Edward\n",
      "was,\n",
      "unottered, \"you hasmed, each mess of good\n",
      "of her\n",
      "dextreable\n",
      "happinets--shendered it was heard, less to love aeac doaw trad!\n",
      "For HESiMy I d\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/1\n",
      "224324/224324 [==============================] - 619s - loss: 1.2810   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"rary accommodation to\n",
      "yourself--such, in short, as\"\n",
      "rary accommodation to\n",
      "yourself--such, in short, as they were to be the promise of her sister was a manners,\n",
      "who had not the could be so much of every the promise of the country to her any the same of her sister was always saw to her in the world.\n",
      "\n",
      "\"I have a mother was in the world to be so much of her sister was a moment to her sister was so much an entirely for her sister so much so much of the rest of her sister was so surprise to her sister wa\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"rary accommodation to\n",
      "yourself--such, in short, as\"\n",
      "rary accommodation to\n",
      "yourself--such, in short, as she was not a should do so me; and the Middletons, and described it it with her deal,\n",
      "who were to be sure it was the family sure she did shall as she had never she saw my dear with her favourite to the win seen expressed at the park in his presention of his denaral without such a few questally pringily in the world to her favourite was a self-more first might subject her sister, and is in the mos\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"rary accommodation to\n",
      "yourself--such, in short, as\"\n",
      "rary accommodation to\n",
      "yourself--such, in short, as the agnial Elinor hers.  I am so life had\n",
      "testency,\n",
      "had not\n",
      "but stried,\n",
      "and fine would all this house was not from the spare came tempers such imaggented, and no ma the estake haling\n",
      "ill, any anxiess, my\n",
      "entirely.  I have not knew I think of the tenious as to side from Willoughby\n",
      "id a me of her moming in some daughter; and yet you htoderess in itpost, you here\n",
      "three fownablity.\n",
      "Hards\n",
      "her sister a\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"rary accommodation to\n",
      "yourself--such, in short, as\"\n",
      "rary accommodation to\n",
      "yourself--such, in short, as is all. \"Whow it was enhour or glad\n",
      "at\n",
      "your sistLucey body,\n",
      "repleavenge was otherwulety prefresd happines\n",
      "ontening\n",
      "proce,\n",
      "any over an out My out a bacurty of his spart hersbespeptence had event againBf as pubsse.  When she had ever\n",
      "to speam,y ever talking\n",
      "nouber ever Mrs. noh, less.\n",
      "\n",
      "Miss\n",
      "Harger\n",
      "maucany\n",
      "edreadby of made\n",
      "have; even hurdo-myself.  Hur sire-vary;--my weeks\n",
      "a versers were there, The \n"
     ]
    }
   ],
   "source": [
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 3):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_to_ix[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = ix_to_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(cwd + '\\\\JaneAustenKerasModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we be impressed by the results we see above? Certainly the generated text itself does not seem to make any sense. But think about what the RNN has managed to learn. The RNN has not been directly provided any notion of English words; we provide it sequences of unicode characters rather than encodings of tokenized words. This means that when the RNN generates English words like 'the', 'should', 'sister', etc., it is not because the RNN was directly provided with information that these sequences of characters are words; the RNN figured this out itself. This recognition of English words occurs after only a few training epochs. The more epochs one uses in training the model, the better the model gets at learning parts of the English language. For instance, after several training epochs, the model appears to have learned that the article 'the' or the possesive pronoun 'her' commonly follow the preposition 'of', but the reverse cases are unlikely. And although the generated sentences don't make any sense, it's not hard to find sentence fragments that are grammatically correct. In conclusion, given the simplicity of the RNN (a single hidden layer of 128 LSTM units) and the small number of training epochs (I'm running this on a 2.7GHz CPU rather than a GPU), I find the RNN's performance rather remarkable."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
